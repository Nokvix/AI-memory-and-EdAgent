import requests
from bs4 import BeautifulSoup
import json
import time
from typing import List, Dict, Optional
import logging
import re
import random

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class HHParser:
    """Parser for HH.ru job vacancies"""

    def __init__(self, output_file: str = 'vacancies.json'):
        """
        Initialize the parser.

        Args:
            output_file: Path to save JSON output
        """
        self.output_file = output_file
        self.vacancies = []
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                         '(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }

    def fetch_vacancy_links_from_file(self, filename: str) -> List[str]:
        """
        Read vacancy links from a text file (generated by hh_parser.py)

        Args:
            filename: Path to the file with vacancy links

        Returns:
            List of vacancy URLs
        """
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                links = [line.strip() for line in f if line.strip()]
            logger.info(f"Loaded {len(links)} vacancy links from {filename}")
            return links
        except FileNotFoundError:
            logger.error(f"File {filename} not found")
            return []

    def get_vacancy_page(self, url: str) -> Optional[BeautifulSoup]:
        """
        Fetch and parse a vacancy page.

        Args:
            url: Vacancy URL

        Returns:
            BeautifulSoup object or None if request fails
        """
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return BeautifulSoup(response.content, 'lxml')
        except requests.RequestException as e:
            logger.warning(f"Error fetching {url}: {e}")
            return None

    def extract_position_title(self, soup: BeautifulSoup) -> Optional[str]:
        """Extract job title from the page"""
        try:
            title_elem = soup.find('h1', {'data-qa': 'vacancy-title'})
            if title_elem:
                return title_elem.get_text(strip=True)
        except Exception as e:
            logger.debug(f"Error extracting title: {e}")
        return None

    def extract_company_name(self, soup: BeautifulSoup) -> Optional[str]:
        """Extract company name from the page"""
        try:
            company_elem = soup.find('span', {'data-qa': 'vacancy-company-name'})
            if company_elem:
                return company_elem.get_text(strip=True)

            company_link = soup.find('a', {'data-qa': 'vacancy-company-name'})
            if company_link:
                return company_link.get_text(strip=True)
        except Exception as e:
            logger.debug(f"Error extracting company name: {e}")
        return None

    def extract_company_url(self, soup: BeautifulSoup) -> Optional[str]:
        """Extract company page URL from the page"""
        try:
            company_link = soup.find('a', {'data-qa': 'vacancy-company-name'})
            if company_link and company_link.has_attr('href'):
                url = company_link['href']
                if url.startswith('/'):
                    return 'https://hh.ru' + url
                return url
        except Exception as e:
            logger.debug(f"Error extracting company URL: {e}")
        return None

    def extract_description(self, soup: BeautifulSoup) -> Optional[str]:
        """
        Extract vacancy description from the page.
        Looks for div with class g-user-content and data-qa attribute
        """
        try:
            description_elem = soup.find('div', {
                'class': 'g-user-content',
                'data-qa': 'vacancy-description'
            })

            if description_elem:
                text = description_elem.get_text(separator='\n', strip=True)
                return text
        except Exception as e:
            logger.debug(f"Error extracting description: {e}")
        return None

    def extract_key_skills(self, soup):
        try:
            items = soup.select('li[data-qa="skills-element"]')
            if not items:
                return []
            skills = []
            for li in items:
                text = li.get_text(' ', strip=True)
                if text:
                    skills.append(text)

            seen = set()
            skills = [s for s in skills if not (s in seen or seen.add(s))]
            return skills
        except Exception as e:
            self.logger.debug(f'key skills parse error: {e}')
            return []

    def parse_vacancy(self, url: str) -> Optional[Dict]:
        """
        Parse a single vacancy page and extract all required data.

        Args:
            url: Vacancy URL

        Returns:
            Dictionary with vacancy data or None if parsing fails
        """
        logger.info(f"Parsing: {url}")
        soup = self.get_vacancy_page(url)

        if not soup:
            return None

        vacancy_data = {
            'position': self.extract_position_title(soup),
            'company_name': self.extract_company_name(soup),
            'company_url': self.extract_company_url(soup),
            'description': self.extract_description(soup),
            'main_skills': self.extract_key_skills(soup),
            'vacancy_url': url
        }

        return vacancy_data

    def parse_vacancies_from_file(self, vacancy_file: str = 'vacancy_links.txt', 
                                  delay: float = 1.0) -> None:
        """
        Parse all vacancies from a file with links.

        Args:
            vacancy_file: Path to file with vacancy URLs
            delay: Delay between requests in seconds (to be respectful to the server)
        """
        links = self.fetch_vacancy_links_from_file(vacancy_file)

        if not links:
            logger.error("No links to process")
            return

        total_links = len(links)

        for idx, link in enumerate(links, 1):
            logger.info(f"Processing {idx}/{total_links}")

            vacancy_data = self.parse_vacancy(link)
            if vacancy_data:
                self.vacancies.append(vacancy_data)

            time.sleep(delay)

        self.save_to_json()

    def parse_vacancy_direct(self, url: str) -> None:
        """
        Parse a single vacancy URL directly.

        Args:
            url: Vacancy URL
        """
        vacancy_data = self.parse_vacancy(url)
        if vacancy_data:
            self.vacancies.append(vacancy_data)
        self.save_to_json()

    def save_to_json(self) -> None:
        """Save collected vacancies to JSON file"""
        try:
            with open(self.output_file, 'w', encoding='utf-8') as f:
                json.dump(self.vacancies, f, ensure_ascii=False, indent=4)

            logger.info(f"Saved {len(self.vacancies)} vacancies to {self.output_file}")
        except Exception as e:
            logger.error(f"Error saving to JSON: {e}")


if __name__ == "__main__":
    parser = HHParser(output_file='vacancies.json')

    parser.parse_vacancies_from_file(
        vacancy_file='vacancy_links.txt',
        delay=random.randint(2, 5)
    )

